{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNET_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ighUY4vF9tcP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import PIL\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtlcPM7qGxWU"
      },
      "source": [
        "def G_noise(image,mu,sigma):\n",
        "  img=np.array(image).astype(np.int16)\n",
        "  t=np.zeros(img.shape,np.int16)\n",
        "  img=img+cv2.randn(t,mu,sigma)\n",
        "  img[img<0]=0\n",
        "  img[img>255]=255\n",
        "  img = img.astype(np.uint8)\n",
        "  #image = PIL.Image.fromarray(img)\n",
        "  image=img\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-w-Hvcg-cNL"
      },
      "source": [
        "#unet Block\n",
        "def double_conv_layer(in_channels, filter):\n",
        "  return nn.Sequential(\n",
        "  nn.Conv2d(in_channels=in_channels,out_channels=filter,kernel_size=(3,3),padding=(1,1)),\n",
        "  nn.BatchNorm2d(num_features=filter),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Conv2d(in_channels=filter,out_channels=filter,kernel_size=(3,3),padding=(1,1)),\n",
        "  nn.BatchNorm2d(num_features=filter),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Dropout2d(p=0.1))\n",
        "   \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei5RbShbwjTX"
      },
      "source": [
        "#defining the network\n",
        "class unet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(unet,self).__init__()\n",
        "    self.d1=double_conv_layer(1,32)\n",
        "    self.p1=nn.MaxPool2d(kernel_size=2,stride=(2,2))\n",
        "    self.d2=double_conv_layer(32,64)\n",
        "   \n",
        "    self.d3=double_conv_layer(64,128)\n",
        "   \n",
        "    self.d4=double_conv_layer(128,256)\n",
        "    self.d5=double_conv_layer(256,512)    \n",
        "    self.bottle=double_conv_layer(512,1024)\n",
        "\n",
        "    self.u5=nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=(2,2),stride=(2,2))\n",
        "    self.ul5=double_conv_layer(1024,512)\n",
        "    self.u4=nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=(2,2),stride=(2,2))\n",
        "    self.ul4=double_conv_layer(512,256)\n",
        "    self.u3=nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=(2,2),stride=(2,2))\n",
        "    self.ul3=double_conv_layer(256,128)\n",
        "\n",
        "    self.u2=nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=(2,2),stride=(2,2))\n",
        "    self.ul2=double_conv_layer(128,64)\n",
        "\n",
        "    self.u1=nn.ConvTranspose2d(in_channels=64,out_channels=32,kernel_size=(2,2),stride=(2,2))\n",
        "    self.ul1=double_conv_layer(64,32)\n",
        "    \n",
        "    self.ol=nn.Conv2d(in_channels=32,out_channels=2,kernel_size=(1,1))\n",
        "\n",
        "    self.sig=nn.Sigmoid()\n",
        "  def forward(self,X):\n",
        "    down1=self.d1(X)\n",
        "    #print(down1.shape)\n",
        "    pool1=self.p1(down1)\n",
        "    \n",
        "    down2=self.d2(pool1)\n",
        "    pool2=self.p1(down2)\n",
        "    \n",
        "    down3=self.d3(pool2)\n",
        "    pool3=self.p1(down3)\n",
        "\n",
        "    down4=self.d4(pool3)\n",
        "    pool4=self.p1(down4)\n",
        "    \n",
        "    #print(pool4.shape)\n",
        "    down5=self.d5(pool4)\n",
        "    pool5=self.p1(down5)\n",
        "    \n",
        "    bottleneck=self.bottle(pool5)\n",
        "    \n",
        "    up5=self.u5(bottleneck)\n",
        "    up5=torch.cat([up5,down5],dim=1)\n",
        "    up5=self.ul5(up5)\n",
        "    \n",
        "    up4=self.u4(up5)\n",
        "    up4=torch.cat([up4,down4],dim=1)\n",
        "    up4=self.ul4(up4)\n",
        "    \n",
        "    up3=self.u3(up4)\n",
        "    up3=torch.cat([up3,down3],dim=1)\n",
        "    up3=self.ul3(up3)\n",
        "\n",
        "    up2=self.u2(up3)\n",
        "    up2=torch.cat([up2,down2],dim=1)\n",
        "    up2=self.ul2(up2)\n",
        "    \n",
        "    up1=self.u1(up2)\n",
        "    up1=torch.cat([up1,down1],dim=1)\n",
        "    up1=self.ul1(up1)\n",
        "    \n",
        "    out=self.ol(up1)\n",
        "    \n",
        "    out=self.sig(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsiOlLiwBN7W"
      },
      "source": [
        "model=unet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8e3WIVaBQIq",
        "outputId": "8e4d3f75-6cf9-42dc-a0f2-c149b2ceb2b0"
      },
      "source": [
        "x=torch.randn((10,1,128,128))\n",
        "x=model(x)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 1, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3NrHXAw7YG"
      },
      "source": [
        "#getting the important libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXmOPCtrBher",
        "outputId": "9ef082a7-78e1-44d6-f897-e10858d35c7b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odKQ0H3Twtpg"
      },
      "source": [
        "#adding border\n",
        "def add_border(img: np.array, size_x: int = 128, size_y: int = 128) -> (np.array, int, int):\n",
        "    \"\"\"Add border to image, so it will divide window sizes: size_x and size_y\"\"\"\n",
        "    max_y, max_x = img.shape[:2]\n",
        "    border_y = 0\n",
        "    if max_y % size_y != 0:\n",
        "        a = (size_y - (max_y % size_y) + 1)\n",
        "       # print('a=',a)\n",
        "        l=0\n",
        "        r=0\n",
        "        if a%2!=0:\n",
        "          l=a//2\n",
        "          r=a//2\n",
        "        else:\n",
        "          l=a//2\n",
        "          r=(a//2)-1\n",
        "        #print(l,r)\n",
        "        img = cv2.copyMakeBorder(img, l, r, 0, 0, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
        "        \n",
        "    border_x = 0\n",
        "    if max_x % size_x != 0:\n",
        "        a = (size_x - (max_x % size_x) + 1)\n",
        "        l=0\n",
        "        r=0\n",
        "        if a%2!=0:\n",
        "          l=a//2\n",
        "          r=a//2\n",
        "        else:\n",
        "          l=a//2\n",
        "          r=(a//2)-1\n",
        "        #print(l,r)\n",
        "        img = cv2.copyMakeBorder(img, 0, 0, l, r, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
        "    return img, border_y, border_x\n",
        "\n",
        "\n",
        "    #preprocessing\n",
        "def split_img(img: np.array, size_x: int = 128, size_y: int = 128 )-> [np.array]:\n",
        "    \"\"\"Split image to parts (little images).\n",
        "    Walk through the whole image by the window of size size_x * size_y without overlays and\n",
        "    save all parts in list. Images sizes should divide window sizes.\n",
        "    \"\"\"\n",
        "    max_y, max_x = img.shape[:2]\n",
        "    parts = []\n",
        "    curr_y = 0\n",
        "    # TODO: rewrite with generators.\n",
        "    while (curr_y + size_y) <= max_y:\n",
        "        curr_x = 0\n",
        "        while (curr_x + size_x) <= max_x:\n",
        "            parts.append(img[curr_y:curr_y + size_y, curr_x:curr_x + size_x])\n",
        "            curr_x += size_x\n",
        "        curr_y += size_y\n",
        "    return parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jSmq6XGxTI-",
        "outputId": "e5f7bb24-c4ad-4128-e28f-f802c4e2f15f"
      },
      "source": [
        "# getting the images from drive for training\n",
        "\n",
        "get_printed=glob.glob('/content/drive/MyDrive/Siddharth-Binarization/dataset_DIBCO/*')\n",
        "print(len(get_printed))\n",
        "\n",
        "images_in=[]\n",
        "images_gt=[]\n",
        "for paths in get_printed:\n",
        "  n=len(glob.glob(paths+\"/*_in.png\"))\n",
        "  for i in range(1,n+1):\n",
        "    if (i<10):\n",
        "      f_name_in=os.path.join(paths,\"0\"+str(i)+\"_in.png\")\n",
        "      f_name_gt=os.path.join(paths,\"0\"+str(i)+\"_gt.png\")\n",
        "    else :\n",
        "      f_name_in=os.path.join(paths,str(i)+\"_in.png\")\n",
        "      f_name_gt=os.path.join(paths,str(i)+\"_gt.png\")\n",
        "\n",
        "    images_in.append(cv2.imread(f_name_in,cv2.IMREAD_GRAYSCALE))\n",
        "    images_gt.append(cv2.imread(f_name_gt,cv2.IMREAD_GRAYSCALE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vxfrj2VsD57d"
      },
      "source": [
        "a=images_in[0]\n",
        "a=G_noise(a,0,100)\n",
        "cv2_imshow(images_in[0])\n",
        "cv2_imshow(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX0ZjxiExZNP",
        "outputId": "1143670e-5e80-45cc-df9d-d0f799761a7c"
      },
      "source": [
        "#splitting the images into 128X128 images for training\n",
        "images_in=np.array(images_in)\n",
        "images_gt=np.array(images_gt)\n",
        "print(images_gt.shape)\n",
        "\n",
        "in_set=[]\n",
        "gt_set=[]\n",
        "n=images_in.shape[0]\n",
        "for i in range(n):\n",
        "  img_in,_,_=add_border(images_in[i])\n",
        "  img_gt,_,_=add_border(images_gt[i])\n",
        "  parts1=split_img(img_in)\n",
        "  parts2=split_img(img_gt)\n",
        "  for img in parts1:\n",
        "    in_set.append(img)\n",
        "  for img in parts2:\n",
        "    gt_set.append(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znv-PtdDM4ye",
        "outputId": "9a13ec19-8c68-44dc-e08e-27ff141dcd47"
      },
      "source": [
        "cd drive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijs8tSU9APeJ"
      },
      "source": [
        "#this cell \n",
        "\n",
        "'''print(images_gt[1].shape)\n",
        "disp=255-predict(model,images_in[1],device)\n",
        "\n",
        "#cv2.imwrite(\"I1.tiff\",disp)\\\n",
        "#disp.resize(images_in[1].shape)\n",
        "\n",
        "X=disp[6:377,50:1230]\n",
        "cv2.imwrite(\"Ia.png\",X,[cv2.IMWRITE_PNG_BILEVEL, 1])\n",
        "print(X.shape)\n",
        "cv2_imshow(images_in[1])\n",
        "cv2_imshow(X)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5LLGXlrx66a",
        "outputId": "6f87f457-5765-4f6c-f08d-d10b905a58e7"
      },
      "source": [
        "# resizing the tensor to pytorch format for training in pytorch\n",
        "in_set=np.array(in_set)\n",
        "gt_set=np.array(gt_set)\n",
        "#print(gt_set.shape)\n",
        "\n",
        "num_samples=in_set.shape[0]\n",
        "in_set.resize((num_samples,1,128,128))\n",
        "#gt_set.resize((num_samples,1,128,128))\n",
        "print(in_set.shape)\n",
        "print(gt_set.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2121, 1, 128, 128)\n",
            "(2121, 128, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ_dTw3HyD3z",
        "outputId": "3cb5412c-4764-4c00-d227-9375c30726ab"
      },
      "source": [
        "# transferring the modle to the GPU\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwErPtPwySRM"
      },
      "source": [
        "#function to invert the images\n",
        "def invert(gt_set):\n",
        "  for i,im in enumerate(gt_set):\n",
        "    #print(i)\n",
        "    gt_set[i]=255-gt_set[i]\n",
        "  return gt_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "v4Gj5EwiyWCS",
        "outputId": "4b8504b9-c294-4f01-89e2-5f5bc5bb269d"
      },
      "source": [
        "#inverting the ground truth images\n",
        "gt_set=invert(gt_set)\n",
        "cv2_imshow(gt_set[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c6c7909f9256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#inverting the ground truth images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgt_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many dimensions: %d > %d.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2694\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z7T5PJZyYZl"
      },
      "source": [
        "#normalizing the pixel valuse\n",
        "in_set=in_set/255\n",
        "gt_set=gt_set/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL2OdIiK53Bh"
      },
      "source": [
        "#getting the test set\n",
        "in_test=in_set[2021:2121]\n",
        "gt_test=gt_set[2021:2121]\n",
        "in_set=in_set[0:2021]\n",
        "gt_set=gt_set[0:2021]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAiYuXCI54r3"
      },
      "source": [
        "#transferring all the tensors to the gpu\n",
        "in_test=torch.tensor(in_test,device=device,dtype=torch.float32)\n",
        "gt_test=torch.tensor(gt_test,device=device,dtype=torch.long)\n",
        "in_set=torch.tensor(in_set,device=device,dtype=torch.float32)\n",
        "gt_set=torch.tensor(gt_set,device=device,dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYx_uLW6yyfX"
      },
      "source": [
        "#preparation for training\n",
        "lr=0.001  #learning rate\n",
        "batch_size=32\n",
        "\n",
        "#loss_func=nn.CrossEntropyLoss()\n",
        "opti=optim.Adam(model.parameters(),lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbJB6_zWzLAB",
        "outputId": "5d72d6b5-9c69-42c4-834e-dd04c9cea042"
      },
      "source": [
        "#generating a single batch for the model to overfit for checking the correctness\n",
        "'''ints=in_set[0:32].to(device=device)\n",
        "gtts=gt_set[0:32].to(device=device)\n",
        "print(ints.shape)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFOW0nu9z5KC"
      },
      "source": [
        "#over fitting the batch\n",
        "'''\n",
        "for i in range(500):\n",
        "  scores=model(ints)\n",
        "  target=gtts\n",
        "  l=loss_func(scores,target)\n",
        "  opti.zero_grad()\n",
        "  #print(i)\n",
        "  l.backward()\n",
        "  opti.step()\n",
        "\n",
        "  if i%20 is 0:\n",
        "    print(l.item())'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGuoTBM-JW6f"
      },
      "source": [
        "def dice_loss(input,target):\n",
        "    # print(input.shape)\n",
        "    # print(target.shape)\n",
        "    assert input.size() == target.size(), \"Input sizes must be equal.\"\n",
        "    assert input.dim() == 4, \"Input must be a 4D Tensor.\"\n",
        "    uniques=np.unique(target.cpu().numpy())\n",
        "    assert set(list(uniques))<=set([0,1]), \"target must only contain zeros and ones\"\n",
        "    probs=input\n",
        "    num=probs*target#b,c,h,w--p*g\n",
        "    num=torch.sum(num,dim=3)#b,c,h\n",
        "    num=torch.sum(num,dim=2)\n",
        "    \n",
        "\n",
        "    den1=probs*probs#--p^2\n",
        "    den1=torch.sum(den1,dim=3)#b,c,h\n",
        "    den1=torch.sum(den1,dim=2)\n",
        "    \n",
        "\n",
        "    den2=target*target#--g^2\n",
        "    den2=torch.sum(den2,dim=3)#b,c,h\n",
        "    den2=torch.sum(den2,dim=2)#b,c\n",
        "    \n",
        "\n",
        "    dice=2*((num+1)/(den1+den2+1))\n",
        "    dice_eso=dice#we ignore bg dice val, and take the fg\n",
        "    #print(dice_eso.shape)\n",
        "    dice_total=-1*torch.sum(dice_eso)/dice_eso.size(0)#divide by batch_sz\n",
        "\n",
        "    return dice_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmfq3-eO5Y_q",
        "outputId": "1e91bc6e-ea07-4757-cf19-eebfd93f09f1"
      },
      "source": [
        "\n",
        "#training the model for num_epochs\n",
        "num_epochs=5\n",
        "for epoch in range(num_epochs):\n",
        "  #perm=torch.randperm(in_set.size()[0])\n",
        "  #perm=perm-1\n",
        "  n=in_set.size()[0]\n",
        "  print(epoch)\n",
        "  for i in range(0,n,batch_size):\n",
        "    opti.zero_grad()\n",
        "    #indices=perm[i:i+batch_size]\n",
        "    \n",
        "    X=in_set[i:i+batch_size]\n",
        "   \n",
        "    scores=model.forward(X)\n",
        "    loss=dice_loss(scores,gt_set[i:i+batch_size])\n",
        "\n",
        "    loss.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if i%1024 is 0:\n",
        "      print(loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "-0.8936399221420288\n",
            "-1.442654013633728\n",
            "1\n",
            "-0.9224237203598022\n",
            "-1.4201297760009766\n",
            "2\n",
            "-0.8833965063095093\n",
            "-1.4574799537658691\n",
            "3\n",
            "-0.8863343596458435\n",
            "-1.4574757814407349\n",
            "4\n",
            "-0.958027720451355\n",
            "-1.4664568901062012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RpBvdLihAFn"
      },
      "source": [
        "\n",
        "#training the model for num_epochs\n",
        "num_epochs=5\n",
        "for epoch in range(num_epochs):\n",
        "  #perm=torch.randperm(in_set.size()[0])\n",
        "  #perm=perm-1\n",
        "  n=in_set.size()[0]\n",
        "  print(epoch)\n",
        "  for i in range(0,n,batch_size):\n",
        "    opti.zero_grad()\n",
        "    #indices=perm[i:i+batch_size]\n",
        "    \n",
        "    X=in_set[i:i+batch_size]\n",
        "   \n",
        "    scores=model.forward(X)\n",
        "    loss=loss_func(scores,gt_set[i:i+batch_size])\n",
        "\n",
        "    loss.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if i%1024 is 0:\n",
        "      print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Ho-awIKgXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a92c73ba-0910-4abb-cace-5168fba5ab4f"
      },
      "source": [
        "#checking accuracy\n",
        "x=model.forward(in_test[0:32])\n",
        "a,b=torch.max(x.data,dim=1,keepdims=True)\n",
        "sz=b.shape[0]\n",
        "num=0\n",
        "for i in range(sz):\n",
        "  num=num+b[i][0].eq(gt_test[i]).sum().item()\n",
        "denom+=sz*128*128\n",
        "print(num/denom)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-c18cf47ea5b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdenom\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'denom' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeGlYMmtA9OS"
      },
      "source": [
        "#saving the model weights after training\n",
        "#torch.save(model.state_dict(),'/content/drive/My Drive/Siddharth-Binarization/model_5eps.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQzYvg7_DMKP"
      },
      "source": [
        "# loading the weights\n",
        "m1=torch.load('/content/drive/My Drive/Siddharth-Binarization/model_5eps.pt')\n",
        "model=unet()\n",
        "model.load_state_dict(m1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxkH53uSLzG",
        "outputId": "fcf9c173-db07-409a-ef33-d38bd2bff53b"
      },
      "source": [
        "get_hr=glob.glob('/content/drive/My Drive/Siddharth-Binarization/hand_written/*')\n",
        "print(get_hr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/hand_written/03_in.png', '/content/drive/My Drive/hand_written/03_gt.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoJWDIF7Kaq3"
      },
      "source": [
        "#getting the handwritten image\n",
        "in_wr=cv2.imread(get_hr[0],cv2.IMREAD_GRAYSCALE)\n",
        "gt_wr=cv2.imread(get_hr[1],cv2.IMREAD_GRAYSCALE)\n",
        "in_wr=np.array(in_wr)\n",
        "gt_wr=np.array(gt_wr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jupYDL7cKhlh"
      },
      "source": [
        "in_wr,_,_=add_border(in_wr)\n",
        "gt_wr,_,_=add_border(gt_wr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwU0nukuKku_"
      },
      "source": [
        "iwr_set=split_img(in_wr)\n",
        "iwr_set=np.array(iwr_set)\n",
        "gwt_set=split_img(gt_wr)\n",
        "gwt_set=np.array(gwt_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPu-Q4MVKnfn"
      },
      "source": [
        "iwr_set=np.resize(iwr_set,(iwr_set.shape[0],1,iwr_set.shape[1],iwr_set.shape[2]))\n",
        "iwr_set=torch.tensor(iwr_set,device=device,dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvOQNq5bIffD"
      },
      "source": [
        "#function to generate a binarized image, given the gray scale image\n",
        "def predict(mode,image,device):\n",
        "  #print(image.shape)\n",
        "  image,_,_=add_border(image)\n",
        "  image=image/255\n",
        " # print(image.shape)\n",
        "  H=image.shape[0]\n",
        "  W=image.shape[1]\n",
        "  \n",
        "  img_set=split_img(image)\n",
        "  img_set=np.array(img_set)\n",
        "  # print(img_set.shape)\n",
        "  img_set=np.resize(img_set,(img_set.shape[0],1,img_set.shape[1],img_set.shape[2]))\n",
        "  img_set=torch.tensor(img_set,device=device,dtype=torch.float32)\n",
        "  \n",
        "  predict=model.forward(img_set)\n",
        "  predict.to('cpu')\n",
        "  \n",
        "  r=predict.shape[0]\n",
        "  _,b=torch.max(predict.data,dim=1,keepdims=True)\n",
        "  #print(b.shape)\n",
        "  b=b.cpu()\n",
        "  predictions=[]\n",
        "  for i in range(r):\n",
        "    #print(i)\n",
        "    predictions.append(b[i][0]*255)\n",
        "  #print(H,W)\n",
        "  i=0\n",
        "  j=0\n",
        "  k=0\n",
        "  ret=np.zeros((H,W))\n",
        "  #print(ret.shape)\n",
        "  while i<H :\n",
        "    j=0\n",
        "    while j<W:\n",
        "      #print(ret[i:i+128,j:j+128].shape)\n",
        "      ret[i:i+128,j:j+128]=predictions[k]\n",
        "      #print(k)\n",
        "      k=k+1\n",
        "      j+=128\n",
        "    i+=128\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "zb5b2kQaHweW",
        "outputId": "0b3d12fc-9381-4ee9-d66d-eb0810a1067b"
      },
      "source": [
        "#displaying a test image\n",
        "#please run this cell after loading the model weights and loading the images, you can change the value of ind variable to change images\n",
        "ind=0\n",
        "disp=predict(model,images_in[ind],device)\n",
        "cv2_imshow(disp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABYAAAAGACAAAAAAxdZnhAAACI0lEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8AxB+AABeUb7/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=1408x384 at 0x7F91AD038208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "cFZwNeh_JnLT",
        "outputId": "cea4f9a7-79c5-4f75-e7fb-0f8253ab3646"
      },
      "source": [
        "#displaying the output of the handwritten image\n",
        "r=predict(model,in_wr,device)\n",
        "cv2_imshow(255-r)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=640x512 at 0x7F4D569F10F0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-09368e503d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_wr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC4r-Rh0In3i"
      },
      "source": [
        "#function to calculate the accuracy , img1 is the ground truth\n",
        "def accuracy(img1,img2):\n",
        "  img1=torch.tensor(img1)\n",
        "  img2=torch.tensor(img2)\n",
        "  num=img1.eq(img2).sum().item()\n",
        "  denom=img1.shape[0]*img1.shape[1]\n",
        "  ans=num/denom\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}